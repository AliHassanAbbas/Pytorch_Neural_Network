{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms"
      ],
      "metadata": {
        "id": "SXGb-fL3smwZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=datasets.FashionMNIST(root='../dataset',train=True,transform=transforms.ToTensor(),download=True)\n",
        "test_dataset=datasets.FashionMNIST(root='../dataset',train=False,transform=transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gX2irdwsmyq",
        "outputId": "7255837d-b08a-4a28-e8f0-01a91c756a41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 9765559.64it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../dataset/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 169871.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../dataset/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3173463.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../dataset/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../dataset/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 18205967.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../dataset/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../dataset/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=64\n",
        "num_epochs=50\n",
        "learning_rate=1e-3\n",
        "device=torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "4VtFLNjAuqqG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader=DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "test_loader=DataLoader(test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "metadata": {
        "id": "7PGA4Ez4sm0j"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class neuralNetwork(nn.Module):\n",
        "  def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n",
        "    super().__init__()\n",
        "    self.layer1=nn.Sequential(\n",
        "        nn.Linear(in_dim,n_hidden_1),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.layer2=nn.Sequential(\n",
        "        nn.Linear(n_hidden_1,n_hidden_2),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "    self.layer3=nn.Sequential(\n",
        "        nn.Linear(n_hidden_2,out_dim),\n",
        "        nn.ReLU(True)\n",
        "        )\n",
        "  def forward(self,x):\n",
        "    x=self.layer1(x)\n",
        "    x=self.layer2(x)\n",
        "    x=self.layer3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "FBLv0A0ssm3B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=neuralNetwork(28 * 28,300,100,10)"
      ],
      "metadata": {
        "id": "UlEc8GMusm5U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if device:\n",
        "  model=model.cuda()"
      ],
      "metadata": {
        "id": "6X4yoPY5sm7W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)"
      ],
      "metadata": {
        "id": "sD9B0kvDsm9c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  print('*' * 10)\n",
        "  print(f'epoch{epoch+1}')\n",
        "  running_loss=0.0\n",
        "  running_acc=0.0\n",
        "  for i,data in enumerate(train_loader,1):\n",
        "    img,label=data\n",
        "    img=img.view(img.size(0),-1)\n",
        "    if device:\n",
        "      img=img.cuda()\n",
        "      label=label.cuda()\n",
        "    out=model(img)\n",
        "    loss=criterion(out,label)\n",
        "    running_loss+=loss.item()\n",
        "    _,pred=torch.max(out,1)\n",
        "    running_acc+=(pred==label).float().mean()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if i % 300 ==0:\n",
        "      print(f'epoch: {epoch+1}/{num_epochs},loss: {running_loss/i:.6f},acc: {running_acc/i:.6f}')\n",
        "  print(f'Finish:{epoch+1} epoch,loss: {running_loss/i:.6f},acc: {running_acc/i:.6f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lv1jvYPsm_j",
        "outputId": "18b833d1-efed-4024-f26f-637dd7fa15c1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********\n",
            "epoch1\n",
            "epoch: 1/50,loss: 1.022195,acc: 0.655052\n",
            "epoch: 1/50,loss: 0.925603,acc: 0.676927\n",
            "epoch: 1/50,loss: 0.877534,acc: 0.689375\n",
            "Finish:1 epoch,loss: 0.874156,acc: 0.690099\n",
            "**********\n",
            "epoch2\n",
            "epoch: 2/50,loss: 0.764056,acc: 0.717344\n",
            "epoch: 2/50,loss: 0.754316,acc: 0.720573\n",
            "epoch: 2/50,loss: 0.745170,acc: 0.723785\n",
            "Finish:2 epoch,loss: 0.745429,acc: 0.723464\n",
            "**********\n",
            "epoch3\n",
            "epoch: 3/50,loss: 0.597815,acc: 0.801042\n",
            "epoch: 3/50,loss: 0.580986,acc: 0.803724\n",
            "epoch: 3/50,loss: 0.573987,acc: 0.804948\n",
            "Finish:3 epoch,loss: 0.572239,acc: 0.805504\n",
            "**********\n",
            "epoch4\n",
            "epoch: 4/50,loss: 0.530607,acc: 0.816042\n",
            "epoch: 4/50,loss: 0.525092,acc: 0.818906\n",
            "epoch: 4/50,loss: 0.526865,acc: 0.817830\n",
            "Finish:4 epoch,loss: 0.526986,acc: 0.817747\n",
            "**********\n",
            "epoch5\n",
            "epoch: 5/50,loss: 0.507967,acc: 0.822188\n",
            "epoch: 5/50,loss: 0.512970,acc: 0.819818\n",
            "epoch: 5/50,loss: 0.508572,acc: 0.821632\n",
            "Finish:5 epoch,loss: 0.507721,acc: 0.821978\n",
            "**********\n",
            "epoch6\n",
            "epoch: 6/50,loss: 0.483182,acc: 0.831823\n",
            "epoch: 6/50,loss: 0.485130,acc: 0.829531\n",
            "epoch: 6/50,loss: 0.491091,acc: 0.826441\n",
            "Finish:6 epoch,loss: 0.490404,acc: 0.826676\n",
            "**********\n",
            "epoch7\n",
            "epoch: 7/50,loss: 0.475335,acc: 0.832188\n",
            "epoch: 7/50,loss: 0.478249,acc: 0.829844\n",
            "epoch: 7/50,loss: 0.477430,acc: 0.829670\n",
            "Finish:7 epoch,loss: 0.477895,acc: 0.829691\n",
            "**********\n",
            "epoch8\n",
            "epoch: 8/50,loss: 0.466110,acc: 0.832292\n",
            "epoch: 8/50,loss: 0.465161,acc: 0.833672\n",
            "epoch: 8/50,loss: 0.467468,acc: 0.833247\n",
            "Finish:8 epoch,loss: 0.467714,acc: 0.833089\n",
            "**********\n",
            "epoch9\n",
            "epoch: 9/50,loss: 0.447851,acc: 0.837083\n",
            "epoch: 9/50,loss: 0.456825,acc: 0.836615\n",
            "epoch: 9/50,loss: 0.459381,acc: 0.835035\n",
            "Finish:9 epoch,loss: 0.459806,acc: 0.835038\n",
            "**********\n",
            "epoch10\n",
            "epoch: 10/50,loss: 0.440882,acc: 0.841146\n",
            "epoch: 10/50,loss: 0.451544,acc: 0.837031\n",
            "epoch: 10/50,loss: 0.451594,acc: 0.837726\n",
            "Finish:10 epoch,loss: 0.450875,acc: 0.838053\n",
            "**********\n",
            "epoch11\n",
            "epoch: 11/50,loss: 0.432642,acc: 0.843854\n",
            "epoch: 11/50,loss: 0.436011,acc: 0.842188\n",
            "epoch: 11/50,loss: 0.441025,acc: 0.841094\n",
            "Finish:11 epoch,loss: 0.441551,acc: 0.840818\n",
            "**********\n",
            "epoch12\n",
            "epoch: 12/50,loss: 0.435775,acc: 0.843385\n",
            "epoch: 12/50,loss: 0.438074,acc: 0.841979\n",
            "epoch: 12/50,loss: 0.433512,acc: 0.843437\n",
            "Finish:12 epoch,loss: 0.433304,acc: 0.843317\n",
            "**********\n",
            "epoch13\n",
            "epoch: 13/50,loss: 0.427516,acc: 0.843021\n",
            "epoch: 13/50,loss: 0.425888,acc: 0.844271\n",
            "epoch: 13/50,loss: 0.428448,acc: 0.843455\n",
            "Finish:13 epoch,loss: 0.428681,acc: 0.843567\n",
            "**********\n",
            "epoch14\n",
            "epoch: 14/50,loss: 0.421736,acc: 0.847188\n",
            "epoch: 14/50,loss: 0.418928,acc: 0.848177\n",
            "epoch: 14/50,loss: 0.420144,acc: 0.847639\n",
            "Finish:14 epoch,loss: 0.419684,acc: 0.847648\n",
            "**********\n",
            "epoch15\n",
            "epoch: 15/50,loss: 0.410720,acc: 0.849844\n",
            "epoch: 15/50,loss: 0.411206,acc: 0.849193\n",
            "epoch: 15/50,loss: 0.414574,acc: 0.847899\n",
            "Finish:15 epoch,loss: 0.414715,acc: 0.848131\n",
            "**********\n",
            "epoch16\n",
            "epoch: 16/50,loss: 0.392782,acc: 0.854323\n",
            "epoch: 16/50,loss: 0.401081,acc: 0.851979\n",
            "epoch: 16/50,loss: 0.407544,acc: 0.850208\n",
            "Finish:16 epoch,loss: 0.407460,acc: 0.850147\n",
            "**********\n",
            "epoch17\n",
            "epoch: 17/50,loss: 0.396215,acc: 0.855417\n",
            "epoch: 17/50,loss: 0.403496,acc: 0.851589\n",
            "epoch: 17/50,loss: 0.403011,acc: 0.851250\n",
            "Finish:17 epoch,loss: 0.401188,acc: 0.851929\n",
            "**********\n",
            "epoch18\n",
            "epoch: 18/50,loss: 0.395956,acc: 0.854219\n",
            "epoch: 18/50,loss: 0.398735,acc: 0.852891\n",
            "epoch: 18/50,loss: 0.397650,acc: 0.853299\n",
            "Finish:18 epoch,loss: 0.397698,acc: 0.853312\n",
            "**********\n",
            "epoch19\n",
            "epoch: 19/50,loss: 0.382750,acc: 0.858438\n",
            "epoch: 19/50,loss: 0.390063,acc: 0.856016\n",
            "epoch: 19/50,loss: 0.393249,acc: 0.854705\n",
            "Finish:19 epoch,loss: 0.393667,acc: 0.854461\n",
            "**********\n",
            "epoch20\n",
            "epoch: 20/50,loss: 0.382742,acc: 0.859115\n",
            "epoch: 20/50,loss: 0.383820,acc: 0.857708\n",
            "epoch: 20/50,loss: 0.388125,acc: 0.856389\n",
            "Finish:20 epoch,loss: 0.388524,acc: 0.856277\n",
            "**********\n",
            "epoch21\n",
            "epoch: 21/50,loss: 0.379064,acc: 0.857760\n",
            "epoch: 21/50,loss: 0.381911,acc: 0.857109\n",
            "epoch: 21/50,loss: 0.381014,acc: 0.858056\n",
            "Finish:21 epoch,loss: 0.381376,acc: 0.857992\n",
            "**********\n",
            "epoch22\n",
            "epoch: 22/50,loss: 0.372454,acc: 0.861250\n",
            "epoch: 22/50,loss: 0.377568,acc: 0.859401\n",
            "epoch: 22/50,loss: 0.376480,acc: 0.859688\n",
            "Finish:22 epoch,loss: 0.377760,acc: 0.858975\n",
            "**********\n",
            "epoch23\n",
            "epoch: 23/50,loss: 0.355601,acc: 0.866146\n",
            "epoch: 23/50,loss: 0.366320,acc: 0.862760\n",
            "epoch: 23/50,loss: 0.372241,acc: 0.860104\n",
            "Finish:23 epoch,loss: 0.371896,acc: 0.860225\n",
            "**********\n",
            "epoch24\n",
            "epoch: 24/50,loss: 0.363973,acc: 0.862865\n",
            "epoch: 24/50,loss: 0.368342,acc: 0.861641\n",
            "epoch: 24/50,loss: 0.369942,acc: 0.861701\n",
            "Finish:24 epoch,loss: 0.369944,acc: 0.861607\n",
            "**********\n",
            "epoch25\n",
            "epoch: 25/50,loss: 0.364998,acc: 0.862135\n",
            "epoch: 25/50,loss: 0.366530,acc: 0.862292\n",
            "epoch: 25/50,loss: 0.366305,acc: 0.862882\n",
            "Finish:25 epoch,loss: 0.365836,acc: 0.863073\n",
            "**********\n",
            "epoch26\n",
            "epoch: 26/50,loss: 0.349606,acc: 0.867917\n",
            "epoch: 26/50,loss: 0.355798,acc: 0.866276\n",
            "epoch: 26/50,loss: 0.362820,acc: 0.864236\n",
            "Finish:26 epoch,loss: 0.362420,acc: 0.864356\n",
            "**********\n",
            "epoch27\n",
            "epoch: 27/50,loss: 0.351451,acc: 0.868125\n",
            "epoch: 27/50,loss: 0.353591,acc: 0.866745\n",
            "epoch: 27/50,loss: 0.358329,acc: 0.865174\n",
            "Finish:27 epoch,loss: 0.359105,acc: 0.864739\n",
            "**********\n",
            "epoch28\n",
            "epoch: 28/50,loss: 0.359662,acc: 0.864427\n",
            "epoch: 28/50,loss: 0.355485,acc: 0.865677\n",
            "epoch: 28/50,loss: 0.354482,acc: 0.866076\n",
            "Finish:28 epoch,loss: 0.355570,acc: 0.865555\n",
            "**********\n",
            "epoch29\n",
            "epoch: 29/50,loss: 0.355909,acc: 0.865573\n",
            "epoch: 29/50,loss: 0.353171,acc: 0.866406\n",
            "epoch: 29/50,loss: 0.350923,acc: 0.867517\n",
            "Finish:29 epoch,loss: 0.350946,acc: 0.867304\n",
            "**********\n",
            "epoch30\n",
            "epoch: 30/50,loss: 0.345839,acc: 0.867917\n",
            "epoch: 30/50,loss: 0.349716,acc: 0.867031\n",
            "epoch: 30/50,loss: 0.349057,acc: 0.867326\n",
            "Finish:30 epoch,loss: 0.348774,acc: 0.867371\n",
            "**********\n",
            "epoch31\n",
            "epoch: 31/50,loss: 0.337940,acc: 0.872448\n",
            "epoch: 31/50,loss: 0.339064,acc: 0.871719\n",
            "epoch: 31/50,loss: 0.347618,acc: 0.867986\n",
            "Finish:31 epoch,loss: 0.347172,acc: 0.868137\n",
            "**********\n",
            "epoch32\n",
            "epoch: 32/50,loss: 0.340461,acc: 0.870781\n",
            "epoch: 32/50,loss: 0.338731,acc: 0.870911\n",
            "epoch: 32/50,loss: 0.340648,acc: 0.870052\n",
            "Finish:32 epoch,loss: 0.340310,acc: 0.870103\n",
            "**********\n",
            "epoch33\n",
            "epoch: 33/50,loss: 0.333339,acc: 0.872187\n",
            "epoch: 33/50,loss: 0.338135,acc: 0.871250\n",
            "epoch: 33/50,loss: 0.341933,acc: 0.869392\n",
            "Finish:33 epoch,loss: 0.340605,acc: 0.869903\n",
            "**********\n",
            "epoch34\n",
            "epoch: 34/50,loss: 0.332442,acc: 0.871615\n",
            "epoch: 34/50,loss: 0.335377,acc: 0.870703\n",
            "epoch: 34/50,loss: 0.335713,acc: 0.870990\n",
            "Finish:34 epoch,loss: 0.335415,acc: 0.871069\n",
            "**********\n",
            "epoch35\n",
            "epoch: 35/50,loss: 0.336610,acc: 0.870885\n",
            "epoch: 35/50,loss: 0.338322,acc: 0.870000\n",
            "epoch: 35/50,loss: 0.337507,acc: 0.870556\n",
            "Finish:35 epoch,loss: 0.336900,acc: 0.870819\n",
            "**********\n",
            "epoch36\n",
            "epoch: 36/50,loss: 0.335185,acc: 0.872187\n",
            "epoch: 36/50,loss: 0.329776,acc: 0.873359\n",
            "epoch: 36/50,loss: 0.334523,acc: 0.872240\n",
            "Finish:36 epoch,loss: 0.333826,acc: 0.872435\n",
            "**********\n",
            "epoch37\n",
            "epoch: 37/50,loss: 0.315580,acc: 0.877813\n",
            "epoch: 37/50,loss: 0.322037,acc: 0.875859\n",
            "epoch: 37/50,loss: 0.326358,acc: 0.874045\n",
            "Finish:37 epoch,loss: 0.326709,acc: 0.873917\n",
            "**********\n",
            "epoch38\n",
            "epoch: 38/50,loss: 0.323435,acc: 0.875104\n",
            "epoch: 38/50,loss: 0.327471,acc: 0.874063\n",
            "epoch: 38/50,loss: 0.329540,acc: 0.873351\n",
            "Finish:38 epoch,loss: 0.330427,acc: 0.872951\n",
            "**********\n",
            "epoch39\n",
            "epoch: 39/50,loss: 0.322899,acc: 0.874427\n",
            "epoch: 39/50,loss: 0.322876,acc: 0.874740\n",
            "epoch: 39/50,loss: 0.327176,acc: 0.873906\n",
            "Finish:39 epoch,loss: 0.327212,acc: 0.873817\n",
            "**********\n",
            "epoch40\n",
            "epoch: 40/50,loss: 0.330320,acc: 0.872187\n",
            "epoch: 40/50,loss: 0.322438,acc: 0.874740\n",
            "epoch: 40/50,loss: 0.320322,acc: 0.876007\n",
            "Finish:40 epoch,loss: 0.321171,acc: 0.875600\n",
            "**********\n",
            "epoch41\n",
            "epoch: 41/50,loss: 0.313993,acc: 0.878125\n",
            "epoch: 41/50,loss: 0.317491,acc: 0.876719\n",
            "epoch: 41/50,loss: 0.319341,acc: 0.875694\n",
            "Finish:41 epoch,loss: 0.319469,acc: 0.875816\n",
            "**********\n",
            "epoch42\n",
            "epoch: 42/50,loss: 0.321202,acc: 0.875573\n",
            "epoch: 42/50,loss: 0.324734,acc: 0.874089\n",
            "epoch: 42/50,loss: 0.320650,acc: 0.876024\n",
            "Finish:42 epoch,loss: 0.321133,acc: 0.875783\n",
            "**********\n",
            "epoch43\n",
            "epoch: 43/50,loss: 0.325543,acc: 0.874010\n",
            "epoch: 43/50,loss: 0.317587,acc: 0.877005\n",
            "epoch: 43/50,loss: 0.321854,acc: 0.875660\n",
            "Finish:43 epoch,loss: 0.322073,acc: 0.875533\n",
            "**********\n",
            "epoch44\n",
            "epoch: 44/50,loss: 0.311471,acc: 0.879635\n",
            "epoch: 44/50,loss: 0.316136,acc: 0.877630\n",
            "epoch: 44/50,loss: 0.320081,acc: 0.875590\n",
            "Finish:44 epoch,loss: 0.318907,acc: 0.876033\n",
            "**********\n",
            "epoch45\n",
            "epoch: 45/50,loss: 0.306477,acc: 0.878958\n",
            "epoch: 45/50,loss: 0.308652,acc: 0.878490\n",
            "epoch: 45/50,loss: 0.312093,acc: 0.877865\n",
            "Finish:45 epoch,loss: 0.313295,acc: 0.877582\n",
            "**********\n",
            "epoch46\n",
            "epoch: 46/50,loss: 0.308859,acc: 0.878021\n",
            "epoch: 46/50,loss: 0.313487,acc: 0.876536\n",
            "epoch: 46/50,loss: 0.314434,acc: 0.876858\n",
            "Finish:46 epoch,loss: 0.314179,acc: 0.876999\n",
            "**********\n",
            "epoch47\n",
            "epoch: 47/50,loss: 0.313509,acc: 0.877917\n",
            "epoch: 47/50,loss: 0.312800,acc: 0.878177\n",
            "epoch: 47/50,loss: 0.311163,acc: 0.878733\n",
            "Finish:47 epoch,loss: 0.311908,acc: 0.878332\n",
            "**********\n",
            "epoch48\n",
            "epoch: 48/50,loss: 0.303775,acc: 0.880365\n",
            "epoch: 48/50,loss: 0.312439,acc: 0.878177\n",
            "epoch: 48/50,loss: 0.312199,acc: 0.878003\n",
            "Finish:48 epoch,loss: 0.311981,acc: 0.878165\n",
            "**********\n",
            "epoch49\n",
            "epoch: 49/50,loss: 0.301660,acc: 0.881250\n",
            "epoch: 49/50,loss: 0.303359,acc: 0.880781\n",
            "epoch: 49/50,loss: 0.307355,acc: 0.879896\n",
            "Finish:49 epoch,loss: 0.307718,acc: 0.879748\n",
            "**********\n",
            "epoch50\n",
            "epoch: 50/50,loss: 0.297992,acc: 0.882083\n",
            "epoch: 50/50,loss: 0.305306,acc: 0.879714\n",
            "epoch: 50/50,loss: 0.308238,acc: 0.878872\n",
            "Finish:50 epoch,loss: 0.308646,acc: 0.878681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "eval_loss=0.0\n",
        "eval_acc=0.0\n",
        "for data in train_loader:\n",
        "  img,label=data\n",
        "  img=img.view(img.size(0),-1)\n",
        "  if device:\n",
        "    img=img.cuda()\n",
        "    label=label.cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out=model(img)\n",
        "    loss=criterion(out,label)\n",
        "    eval_loss+=loss.item()\n",
        "    _,pred=torch.max(out,1)\n",
        "    eval_acc+=(pred==label).float().mean()\n",
        "print(f'Test loss: {eval_loss/len(train_loader)},acc: {eval_acc/len(train_loader)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYzaUFa6snD7",
        "outputId": "26396caa-204d-4a48-ba41-6e05add78c65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.3154634066871298,acc: 0.8729677796363831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(),'./neuralNetwork.pth')"
      ],
      "metadata": {
        "id": "6yE6ybWpsnGG"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}